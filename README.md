ğŸ™ï¸ Speaker Diarization using Fine-Tuned PyAnnote Segmentation
ğŸ“Œ Project Overview

This project implements speaker diarization â€” answering the question:

â€œWho spoke when?â€

The system is built using PyAnnote Audio, where a speaker segmentation model is fine-tuned on custom audio data, followed by speaker embedding extraction and clustering to identify different speakers in an audio file.

This project is suitable for:

1. Call analysis
2. Meeting transcription
3. Surveillance / forensics
4. Conversational AI pipelines
5. Academic research & viva evaluation

ğŸ§  What is Speaker Diarization?

Speaker diarization breaks an audio file into:
Speech vs non-speech
Speaker segments
Speaker labels (Speaker 1, Speaker 2, â€¦)

Example output:
00:00â€“00:12  â†’ Speaker 1
00:12â€“00:20  â†’ Speaker 2
00:20â€“00:35  â†’ Speaker 1

ğŸ—ï¸ Project Architecture
Audio Input
   â†“
Preprocessing (Mono + 16kHz)
   â†“
Fine-Tuned Segmentation Model (PyAnnote)
   â†“
Speech Activity Detection
   â†“
Speaker Embedding Extraction (Speech Brain)
   â†“
Agglomerative Clustering
   â†“
Final Speaker Diarization Output (RTTM / Timeline)

ğŸ”§ Technologies & Libraries Used

Python 3.9+
PyTorch
PyAnnote Audio
Torchaudio
Scikit-learn
NumPy
Matplotlib
RTTM format for evaluation

ğŸ“ Project Structure
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ audio/                 # Training & evaluation audio files
â”‚   â”œâ”€â”€ rttm/                  # Ground truth RTTM files
â”‚   â””â”€â”€ database.yml           # PyAnnote database configuration
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ segmentation/          # Fine-tuned segmentation checkpoints
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_segmentation.py  # Model fine-tuning script
â”‚   â”œâ”€â”€ diarization.py         # Inference + clustering
â”‚   â””â”€â”€ visualize.py           # Diarization visualization
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ rttm/                  # Predicted RTTM files
â”‚   â””â”€â”€ plots/                 # Speaker timeline plots
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ¯ Key Features

âœ… Fine-tuned speaker segmentation model
âœ… Works on real-world conversational audio
âœ… Supports variable number of speakers
âœ… Uses Agglomerative Clustering
âœ… Produces RTTM diarization output
âœ… Visualization of speaker segments

ğŸ”Š Audio Requirements (Important)
All audio must be:
Mono (1 channel)
16 kHz sampling rate
WAV format

ğŸ“Œ Why?
PyAnnote models are trained on mono 16kHz
Stereo or different sampling rates reduce accuracy

ğŸš€ How to Run the Project

1ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Prepare Dataset
Place audio files in dataset/audio/
Place RTTM files in dataset/rttm/
Configure paths in dataset/database.yml

4ï¸âƒ£ Fine-Tune Segmentation Model
python scripts/train_segmentation.py

5ï¸âƒ£ Run Speaker Diarization
python scripts/diarization.py --audio sample.wav

6ï¸âƒ£ Visualize Output
python scripts/visualize.py

ğŸ“Š Output Formats
RTTM file for evaluation
Speaker timeline plots
Segment-level speaker annotations

ğŸ“ˆ Evaluation Metrics
Diarization Error Rate (DER)
False Alarm
Missed Speech
Speaker Confusion

ğŸ§ª Model Details
Base Model: PyAnnote Speaker Segmentation
Training: Supervised fine-tuning
Loss Function: Binary cross-entropy (segmentation)
Clustering: Agglomerative Hierarchical Clustering
Embeddings: Speaker embeddings from PyAnnote pipeline

âš ï¸ Limitations
Overlapping speech is challenging
Performance depends on audio quality
Needs sufficient labeled RTTM data

ğŸŒ± Future Improvements
ğŸ”¹ Overlap-aware diarization
ğŸ”¹ Domain-specific embedding fine-tuning
ğŸ”¹ Real-time diarization
ğŸ”¹ Integration with ASR (speech-to-text)

ğŸ‘©â€ğŸ’» Author
Anamika Pandey
BCA | AI/ML | Speaker Diarization | PyAnnote

ğŸ“œ License
This project is for academic and learning purposes.
Pre-trained models belong to their respective authors.

ğŸ™Œ Acknowledgements
PyAnnote Audio Team

HuggingFace

PyTorch Community
